Llama3.1 405b
https://ollama.com/library/llama3.1

/home/ec2-user/miniconda3/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work

메타의 Llama3.1이 공개되었습니다. 미국쪽은 반응이 뜨겁네요.
405B의 모델을 돌리려면, 컴퓨팅 파워가 매우 많이 필요합니다.

일단 8B짜리로 돌려보겠습니다.

Groq에도 적용이 되었는데, 1시간 전에는 잘 동작하지 않았습니다.
https://groq.com

여튼 aws에 설치해 보겠습니다.
g4dn.8xlarge는 cpu가 32개짜리입니다.
한시간에 3000원 정도죠.
2개의 볼륨 – 930GiB
로 맞춰서 진행합니다.

https://okdevtv.com/mib/ollama 에 스크립트를 이용합니다.

mount 한 볼륨 쪽으로 symbolic link를 연결합니다.

ollama 설치중입니다.
ollama는 공개된 AI 모델을 편하게 관리할 수 있는 오픈 소스입니다.
벌써 2만3천 다운받았네요.
[23.4K Pulls](https://ollama.com/library/llama3.1)

open-webui도 동시에 설치합니다.

ollama 다운 받는 곳도 storage위치를 변경합니다.

ollama pull llama3.1 
로 AI 모델을 다운 받습니다.

비교를 위해서 llama3 도 다운받겠습니다.
